# ESUSSifilis.py
# -*- coding: utf-8 -*-
"""
GestaWeb DS7 â€” e-SUS Monitoramento (DS VII) com login Firebase e bloqueio por UNIDADE
- Login (Firebase Email/Password, via Pyrebase)
- ADMIN Ãºnico pode carregar atÃ© 69 planilhas (CSV/XLS/XLSX/ODS)
- Base Ã© persistida em disco para todos os usuÃ¡rios (data/base.parquet ou CSV de fallback)
- <<< CORRIGIDO: Carrega credenciais do Firestore de um arquivo JSON separado >>>
- UsuÃ¡rios comuns: somente visualizaÃ§Ã£o e apenas da(s) sua(s) UNIDADE(s)
- Detecta UNIDADE (prioriza coluna do arquivo; fallback por heurÃ­stica)
- MantÃ©m TODAS as colunas
- Filtro por UNIDADE (por palavra-chave; ignora o que vem apÃ³s a vÃ­rgula)
- RelatÃ³rios AR/AS/AT/AU/AV/AN/AJ + grÃ¡ficos
"""

import io
import re
import csv
import unicodedata
import warnings
import datetime
from io import StringIO
from typing import List, Tuple, Dict, Optional
from pathlib import Path
from uuid import uuid4  # IDs seguros no Firestore

import pandas as pd
import streamlit as st
import altair as alt
import numpy as np

warnings.simplefilter("ignore", category=UserWarning)
st.set_page_config(page_title="GestaWeb DS7 â€” e-SUS", layout="wide")

# --- DependÃªncias externas ---
try:
    from google.cloud import firestore
    from google.oauth2 import service_account
    import pyrebase
except ImportError:
    st.error("DependÃªncias ausentes! Instale: pip install pyrebase4 google-cloud-firestore")
    st.stop()

# =========================
# Credenciais / ConexÃµes
# =========================
FIRESTORE_CREDENTIALS_PATH = "service_account.json"  # aponte para o seu JSON

FIREBASE_CONFIG = {
    "apiKey": "AIzaSyD7-HwufkeY97nP5I61DZJaeCTNY7ccvKo",
    "authDomain": "gestawebds7raquelacioli.firebaseapp.com",
    "projectId": "gestawebds7raquelacioli",
    "storageBucket": "gestawebds7raquelacioli.appspot.com",
    "messagingSenderId": "536966961533",
    "appId": "1:536966961533:web:1f4b0bfe25f60799c31414",
    "databaseURL": "https://gestawebds7raquelacioli-default-rtdb.firebaseio.com"
}

@st.cache_resource(show_spinner=False)
def firebase_init():
    try:
        return pyrebase.initialize_app(FIREBASE_CONFIG)
    except Exception as e:
        st.error(f"Falha ao inicializar Pyrebase (login): {e}")
        st.stop()

@st.cache_resource(show_spinner=False)
def get_firestore_client():
    try:
        if not Path(FIRESTORE_CREDENTIALS_PATH).is_file():
            st.error("ARQUIVO DE CREDENCIAIS NÃƒO ENCONTRADO!")
            st.info(f"Verifique o caminho em FIRESTORE_CREDENTIALS_PATH: {FIRESTORE_CREDENTIALS_PATH}")
            st.stop()
        creds = service_account.Credentials.from_service_account_file(FIRESTORE_CREDENTIALS_PATH)
        return firestore.Client(credentials=creds)
    except Exception as e:
        st.error(f"Falha ao conectar no Firestore: {e}")
        st.stop()

def firebase_sign_in(firebase, email: str, password: str):
    return firebase.auth().sign_in_with_email_and_password(email, password)

# =========================
# Banner de login (opcional)
# =========================
HERO_CANDIDATES = [
    r"C:\Users\raque\Desktop\Banco Sifilis\assets\gestaweb_ds7.jpg",
    "assets/gestaweb_ds7.jpg",
    "/mnt/data/gestaweb_ds7.jpg",
]
def _pick_hero_path() -> Optional[Path]:
    for p in HERO_CANDIDATES:
        path = Path(p)
        if path.exists():
            return path
    return None
def show_login_banner():
    hero = _pick_hero_path()
    if hero and hero.exists():
        st.image(str(hero), use_container_width=True)

# =========================
# PersistÃªncia local
# =========================
PERSIST_DIR = Path("data")
PERSIST_DIR.mkdir(exist_ok=True)
PERSIST_PATH = PERSIST_DIR / "base.parquet"

def save_base_to_disk(df: pd.DataFrame) -> None:
    try:
        df.to_parquet(PERSIST_PATH, index=False)
    except Exception:
        df.to_csv(PERSIST_PATH.with_suffix(".csv"), index=False, encoding="utf-8-sig")

def load_base_from_disk() -> Optional[pd.DataFrame]:
    if PERSIST_PATH.exists():
        return pd.read_parquet(PERSIST_PATH)
    csv_path = PERSIST_PATH.with_suffix(".csv")
    if csv_path.exists():
        return pd.read_csv(csv_path, dtype=str)
    return None

# =========================
# Firestore helpers
# =========================
def clean_firestore_value(value):
    if pd.isna(value) or value is None:
        return None
    if isinstance(value, (np.int64, np.int32, np.int16)):
        return int(value)
    if isinstance(value, (np.float64, np.float32)):
        return float(value)
    if isinstance(value, (pd.Timestamp, datetime.datetime, datetime.date)):
        return value.to_pydatetime() if isinstance(value, pd.Timestamp) else value
    return str(value)

def sanitize_doc_id(s: str) -> str:
    s = unicodedata.normalize("NFKD", str(s))
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    s = s.strip().lower()
    s = s.replace("/", "-").replace("\\", "-")
    s = re.sub(r"[\[\]#?]+", "-", s)
    s = re.sub(r"\s+", "_", s)
    s = re.sub(r"[^a-z0-9_\-\.]+", "-", s)
    s = s[:150].strip("-.")
    return s or f"doc_{uuid4().hex[:10]}"

def save_to_firestore(df: pd.DataFrame, collection_name: str = "gestantes_sifilis"):
    try:
        db = get_firestore_client()
        st.info(f"Salvando {len(df)} registros em '{collection_name}'...")
        sanitized_columns = {col: re.sub(r'[^a-zA-Z0-9_]', '_', col) for col in df.columns}
        df_sanitized = df.rename(columns=sanitized_columns)
        col_paciente = find_first_matching_col(df_sanitized, COLMAP["paciente"]) or df_sanitized.columns[0]
        unidade_col = "UNIDADE" if "UNIDADE" in df_sanitized.columns else None

        progress_bar = st.progress(0)
        batch = db.batch()
        batch_count = 0
        total = len(df_sanitized)

        for i, row in df_sanitized.iterrows():
            doc = {c: clean_firestore_value(v) for c, v in row.items()}
            paciente_id = sanitize_doc_id(str(doc.get(col_paciente, "")))
            unidade_id  = sanitize_doc_id(str(doc.get(unidade_col, "sem_unidade")))
            doc_id = f"{unidade_id}__{paciente_id}__{i}"
            doc["_timestamp_upload"] = firestore.SERVER_TIMESTAMP
            batch.set(db.collection(collection_name).document(doc_id), doc)
            batch_count += 1
            if batch_count >= 499:
                batch.commit()
                batch = db.batch()
                batch_count = 0
            progress_bar.progress((i + 1) / total)
        if batch_count:
            batch.commit()
        progress_bar.empty()
        st.success("Registros salvos/atualizados com sucesso!")
    except Exception as e:
        st.error(f"Erro ao salvar no Firestore: {e}")
        st.exception(e)

# =========================
# Mapeamentos / critÃ©rios
# =========================
COLMAP = {
    "paciente": [
        "nome do paciente","paciente","nome",
        "nome do usuÃ¡rio","nome do usuario",
        "nome da usuaria","nome da usuÃ¡ria",
        "usuario","usuÃ¡rio"
    ],
}

COL_SPECS = {
    "AR":{"desc":"Exame de HIV no 1Âº trimestre","type":"text_nao",
          "names":["Exame de HIV no primeiro trimestre","hiv 1Âº trimestre","hiv 1 tri"]},
    "AS":{"desc":"Exame de SÃ­filis no 1Âº trimestre","type":"text_nao",
          "names":["Exame de SÃ­filis no primeiro trimestre","sÃ­filis 1Âº trimestre","sifilis 1 tri"]},
    "AT":{"desc":"Exame de Hepatite B no 1Âº trimestre","type":"text_nao",
          "names":["Exame de Hepatite B no primeiro trimestre","hbv 1Âº trimestre","hepatite b 1 tri"]},
    "AU":{"desc":"Exame de Hepatite C no 1Âº trimestre","type":"text_nao",
          "names":["Exame de Hepatite C no primeiro trimestre","hcv 1Âº trimestre","hepatite c 1 tri"]},
    "AV":{"desc":"Exame de HIV no 3Âº trimestre","type":"text_nao",
          "names":["Exame de HIV no terceiro trimestre","hiv 3Âº trimestre","hiv 3 tri"]},
    "AN":{"desc":"Quantidade de atendimentos odontolÃ³gicos no prÃ©-natal","type":"num_lt","value":1,
          "names":["Quantidade de atendimentos odontolÃ³gicos no prÃ©-natal","odontol","atend odont"]},
    "AJ":{"desc":"IG (DUM) (semanas)","type":"num_gt","value":43,
          "names":["IG (DUM) (semanas)","idade gestacional (dum)","ig semanas"]},
}
LABELS = {
    "AR":"Exame de HIV no 1Âº trimestre = NÃƒO",
    "AS":"Exame de SÃ­filis no 1Âº trimestre = NÃƒO",
    "AT":"Exame de Hepatite B no 1Âº trimestre = NÃƒO",
    "AU":"Exame de Hepatite C no 1Âº trimestre = NÃƒO",
    "AV":"Exame de HIV no 3Âº trimestre = NÃƒO",
    "AN":"Quantidade de atendimentos odontolÃ³gicos no prÃ©-natal < 1",
    "AJ":"IG (DUM) (semanas) > 43",
}

# =========================
# ADMIN e acesso por e-mail -> PALAVRAS-CHAVE
# =========================
ADMIN_EMAILS = {"vigilanciaepidemiologicadsvii@gmail.com"}

# Para cada e-mail, informe PALAVRAS-CHAVE (famÃ­lias) da unidade.
# Ex.: "bruno maia" farÃ¡ o usuÃ¡rio ver ESF MAIS BRUNO MAIA I/III/IV/V/VI...
EMAIL_TO_UNITS: Dict[str, List[str]] = {
    "brunomaiatbehansen@gmail.com": ["bruno maia"],
    "passarinhobaixotbehansen@gmail.com": ["passarinho baixo"],
    "passarinhoaltotbehansen@gmail.com": ["passarinho alto"],

    "altodoeucaliptotbehansen@gmail.com": ["alto do eucalipto"],
    "corregodoeucaliptotbehansen@gmail.com": ["corrego do eucalipto"],
    "corregodabicatbehansen@gmail.com": ["corrego da bica"],

    "alcidescodeceiratbehansen@gmail.com": ["alto jose bonifacio"],
    "morrodaconceicaotbehansen@gmail.com": ["morro da conceicao"],
    "mariomonteirotbehansen@gmail.com": ["mario monteiro"],
    "inarosaborgestbehansen@gmail.com": ["ina rosa"],
    "macaxeiratbehansen@gmail.com": ["macaxeira"],
    "mangabeiratbehansen@gmail.com": ["mangabeira"],
    "bolanaredetbehansen@gmail.com": ["bola na rede"],
    "domheldertbehansen@gmail.com": ["dom helder"],
    "heliomendoncatbehansen@gmail.com": ["helio mendonca"],
    "clementinofragatbehansen@gmail.com": ["clementino fraga"],
    "guabirabatbehansen@gmail.com": ["guabiraba"],
    "santaterezatbehansen@gmail.com": ["santa tereza"],
    "moacyrandregomestbehansen@gmail.com": ["moacyr"],
    "vilaboavistatbehansen@gmail.com": ["vila boa vista"],
    "corregodoeuclidestbehansen@gmail.com": ["maria rita"],
    "sitiodosmacacostbehansen01@gmail.com": ["sitios dos macacos","sitio dos macacos"],
    "irmadenisetbehansen@gmail.com": ["irma denise"],
    "diogenescavalcantitbehansen@gmail.com": ["diogenes cavalcanti"],
}

# =========================
# Helpers ETL
# =========================
def normalize(s: str) -> str:
    if not isinstance(s, str):
        s = str(s)
    s = s.strip().lower()
    s = unicodedata.normalize("NFKD", s)
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    s = re.sub(r"\s+", " ", s)
    return s

def find_first_matching_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:
    cols_norm = {normalize(c): c for c in df.columns}
    for c in candidates:
        nc = normalize(c)
        if nc in cols_norm:
            return cols_norm[nc]
    for real in df.columns:
        rn = normalize(real)
        for cand in candidates:
            if normalize(cand) in rn:
                return real
    return None

def _read_csv_robusto(file_obj) -> pd.DataFrame:
    if hasattr(file_obj, "read"):
        pos = file_obj.tell() if hasattr(file_obj, "tell") else None
        data = file_obj.read()
        if pos is not None:
            try: file_obj.seek(pos)
            except Exception: pass
    else:
        with open(file_obj, "rb") as fh:
            data = fh.read()
    if isinstance(data, bytes):
        try: text = data.decode("utf-8","ignore")
        except Exception: text = data.decode("latin-1","ignore")
    else:
        text = str(data)
    buf = StringIO(text)
    try:
        return pd.read_csv(buf, sep=None, engine="python", header=None, dtype=str,
                           skip_blank_lines=True, quoting=csv.QUOTE_MINIMAL, on_bad_lines="skip")
    except Exception:
        pass
    for sep in [";","\t",",","|"]:
        buf.seek(0)
        try:
            return pd.read_csv(buf, sep=sep, engine="python", header=None, dtype=str,
                               skip_blank_lines=True, quoting=csv.QUOTE_MINIMAL, on_bad_lines="skip")
        except Exception:
            continue
    buf.seek(0)
    return pd.read_csv(buf, header=None, dtype=str, engine="python", on_bad_lines="skip")

def detect_unit_name(raw_df: pd.DataFrame) -> Optional[str]:
    try:
        b10 = raw_df.iloc[9, 1]
        if isinstance(b10, str) and b10.strip(): return b10.strip()
    except Exception: pass
    try:
        a3 = raw_df.iloc[2, 0]
        if isinstance(a3, str) and normalize(a3).startswith("unidade de saude"):
            return a3.strip()
    except Exception: pass
    for i in range(min(10, len(raw_df))):
        try:
            v = str(raw_df.iloc[i, 0])
            if re.search(r"(?i)unidade\s+de\s+saud(e|Ã©)", v): return v.strip()
        except Exception: continue
    return None

def find_unit_column(df: pd.DataFrame) -> Optional[str]:
    candidates = [
        "UNIDADE","UNIDADE DE SAÃšDE","UNIDADE DE SAUDE","UNIDADE (BA)","BA",
        "ESTABELECIMENTO","UNIDADE/ESTABELECIMENTO"
    ]
    cols_norm = {normalize(c): c for c in df.columns}
    for c in candidates:
        if normalize(c) in cols_norm: return cols_norm[normalize(c)]
    for real in df.columns:
        if "unidade" in normalize(real) or "estabelec" in normalize(real):
            return real
    return None

def read_any_table(file) -> Tuple[pd.DataFrame, Optional[str]]:
    name = getattr(file, "name", str(file)).lower()
    if name.endswith(".csv"):
        raw = _read_csv_robusto(file)
    elif name.endswith((".xlsx",".xls")):
        raw = pd.read_excel(file, header=None, dtype=str)
    elif name.endswith(".ods"):
        try: raw = pd.read_excel(file, engine="odf", header=None, dtype=str)
        except Exception: raw = pd.read_excel(file, header=None, dtype=str)
    else:
        raw = _read_csv_robusto(file)

    unidade_hint = detect_unit_name(raw)

    # Detecta linha de cabeÃ§alho
    header_row = None
    for i in range(min(30, len(raw))):
        row = raw.iloc[i]
        text_count = sum(len(str(x).strip()) > 0 for x in row.dropna().astype(str))
        if text_count >= 4:
            header_row = i; break
    if header_row is None: header_row = 0

    df = raw.copy()
    df.columns = raw.iloc[header_row].fillna("")
    df = raw.iloc[header_row+1:].reset_index(drop=True)
    df.columns = [str(c).strip() if str(c).strip() != "" else f"col_{i}" for i,c in enumerate(df.columns)]

    col_paciente = find_first_matching_col(df, COLMAP["paciente"]) or df.columns[0]
    df = df[df[col_paciente].notna() & (df[col_paciente].astype(str).str.strip() != "")]

    col_unid = find_unit_column(df)
    if col_unid:
        df["UNIDADE"] = df[col_unid].astype(str).str.strip()
    else:
        df["UNIDADE"] = unidade_hint if unidade_hint else "(UNIDADE NAO DETECTADA)"
    return df, unidade_hint

def excel_letter_to_index(letter: str) -> int:
    if not letter or not letter.isalpha(): return -1
    letter = letter.upper(); idx = 0
    for ch in letter: idx = idx*26 + (ord(ch)-ord('A')+1)
    return idx - 1

def get_col_by_letter_or_name(df: pd.DataFrame, letter: str, names: List[str]) -> Optional[str]:
    j = excel_letter_to_index(letter)
    if 0 <= j < len(df.columns):
        col = df.columns[j]
        if str(col).strip() != "": return col
    names_n = [normalize(n) for n in names]
    for c in df.columns:
        cn = normalize(c)
        for nn in names_n:
            if cn == nn or nn in cn: return c
    return None

def series_is_nao(s: pd.Series) -> pd.Series:
    return s.astype(str).map(normalize).str.fullmatch(r"n[aÃ£]o")

def to_numeric_series(s: pd.Series) -> pd.Series:
    ss = s.astype(str).str.replace(".", "", regex=False).str.replace(",", ".", regex=False)
    return pd.to_numeric(ss, errors="coerce")

def build_requested_filters(df: pd.DataFrame) -> Tuple[Dict[str, pd.Series], Dict[str, str]]:
    masks, found_cols = {}, {}
    for code, spec in COL_SPECS.items():
        col = get_col_by_letter_or_name(df, code, spec.get("names", []))
        if col is None: continue
        found_cols[code] = col
        t = spec["type"]
        if t == "text_nao":
            masks[code] = series_is_nao(df[col])
        elif t == "num_lt":
            x = to_numeric_series(df[col]); masks[code] = x < spec.get("value", 0)
        elif t == "num_gt":
            x = to_numeric_series(df[col]); masks[code] = x > spec.get("value", 0)
    return masks, found_cols

def df_to_csv_bytes(df: pd.DataFrame) -> bytes:
    return df.to_csv(index=False).encode("utf-8-sig")

def single_sheet_xlsx(df: pd.DataFrame, sheet_name: str = "PLANILHA") -> bytes:
    out = io.BytesIO()
    with pd.ExcelWriter(out, engine="openpyxl") as writer:
        df.to_excel(writer, sheet_name=sheet_name[:31], index=False)
    out.seek(0)
    return out.read()

# =========================
# Login (Firebase)
# =========================
def login_block():
    show_login_banner()
    st.title("ðŸ” Login â€” GestaWeb DS7 (Firebase)")
    with st.form("login_form", clear_on_submit=False):
        email_in = st.text_input("E-mail institucional", value="", placeholder="nome@dominio")
        pwd_in   = st.text_input("Senha", value="", type="password")
        ok = st.form_submit_button("Entrar")
    if ok:
        try:
            fb = firebase_init()
            user = firebase_sign_in(fb, email_in.strip(), pwd_in)
            st.session_state["auth_ok"]   = True
            st.session_state["user_email"]= email_in.strip().lower()
            st.session_state["id_token"]  = user.get("idToken")
            # PermissÃµes
            if st.session_state["user_email"] in ADMIN_EMAILS:
                st.session_state["allowed_units"] = ["*"]
            else:
                st.session_state["allowed_units"] = EMAIL_TO_UNITS.get(st.session_state["user_email"], [])
            st.success("Login realizado.")
            st.rerun()
        except Exception as e:
            st.error(f"Falha no login: {e}")
    st.stop()

# Gate de login
if not st.session_state.get("auth_ok"):
    login_block()

# Sidebar
with st.sidebar:
    st.success(f"Conectado: {st.session_state.get('user_email','')}")
    if st.button("Sair"):
        st.session_state.clear()
        st.rerun()

# =========================
# UI principal
# =========================
st.title("Unidades de SaÃºde DSVII â€” GestaWeb DS7 (e-SUS)")

user_email = (st.session_state.get("user_email") or "").lower()
allowed_units: List[str] = st.session_state.get("allowed_units", [])
is_admin = (user_email in ADMIN_EMAILS) or ("*" in allowed_units)

if is_admin:
    st.success("Modo ADMIN: vocÃª pode carregar planilhas e visualizar todas as unidades.")
else:
    st.info("Modo VISUALIZAÃ‡ÃƒO: vocÃª verÃ¡ apenas os dados da(s) sua(s) unidade(s). Upload desabilitado.")

# Upload (ADMIN)
uploaded_files = None
if is_admin:
    uploaded_files = st.file_uploader(
        "Selecione as planilhas (atÃ© 69) â€” apenas ADMIN",
        type=["csv","xlsx","xls","ods"],
        accept_multiple_files=True,
    )
else:
    st.caption("Upload desabilitado para usuÃ¡rios nÃ£o-admin.")

if is_admin and uploaded_files:
    if len(uploaded_files) > 69:
        st.warning("VocÃª selecionou mais de 69 arquivos. Apenas os 69 primeiros serÃ£o processados.")
        uploaded_files = uploaded_files[:69]
    with st.spinner("Lendo e preparando as planilhas..."):
        dfs = []
        for f in uploaded_files:
            try:
                df, _ = read_any_table(f)
                dfs.append(df)
            except Exception as e:
                st.error(f"Erro ao ler {getattr(f,'name','arquivo')}: {e}")
        if not dfs:
            st.stop()
        consolidated_df = pd.concat(dfs, ignore_index=True)
        st.session_state["base_df"] = consolidated_df
        save_base_to_disk(consolidated_df)
        st.success("Base atualizada e salva em disco (disponÃ­vel para todos).")
        save_to_firestore(consolidated_df.copy(), collection_name="gestantes")

# Carrega base (sessÃ£o ou disco)
if "base_df" not in st.session_state:
    disk_df = load_base_from_disk()
    if disk_df is not None:
        st.session_state["base_df"] = disk_df
    else:
        st.warning("Nenhuma base carregada ainda. O ADMIN precisa carregar as planilhas.")
        st.stop()

base = st.session_state["base_df"].copy()

# DiagnÃ³stico: ADMIN vÃª as UNIDADES Ãºnicas
if is_admin and "UNIDADE" in base.columns:
    with st.expander("DiagnÃ³stico: UNIDADES Ãºnicas na base"):
        unidades_unicas = sorted(base["UNIDADE"].dropna().astype(str).str.strip().unique().tolist())
        st.write(unidades_unicas[:300])

# ---------- Filtro por UNIDADE (por palavra-chave) ----------
def _norm_view(s: str) -> str:
    s = unicodedata.normalize("NFKD", str(s))
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    return re.sub(r"\s+", " ", s).strip().lower()

def _limpa_unidade(u: str) -> str:
    # Remove o cÃ³digo apÃ³s a vÃ­rgula (ex.: "ESF MAIS BRUNO MAIA I, 0000152900" -> "ESF MAIS BRUNO MAIA I")
    return str(u).split(",", 1)[0].strip()

def _matches_keywords(unit_text: str, allowed_keywords_norm: set[str]) -> bool:
    nu = _norm_view(_limpa_unidade(unit_text))
    return any(kw in nu for kw in allowed_keywords_norm)

if not is_admin:
    if not allowed_units:
        st.warning("Seu e-mail autenticou, mas nÃ£o estÃ¡ associado a nenhuma UNIDADE. Solicite cadastro.")
        st.stop()
    if "UNIDADE" not in base.columns:
        st.warning("A base carregada nÃ£o possui a coluna 'UNIDADE'.")
        st.stop()

    keywords_norm = {_norm_view(k) for k in allowed_units}
    base = base[ base["UNIDADE"].astype(str).map(lambda x: _matches_keywords(x, keywords_norm)) ]

    if base.empty:
        with st.expander("DiagnÃ³stico do filtro (nÃ£o-admin)"):
            st.write("Suas palavras-chave:", allowed_units)
            st.write("Normalizadas:", sorted(list(keywords_norm)))
            try:
                amostra = sorted({
                    _norm_view(_limpa_unidade(u))
                    for u in st.session_state["base_df"]["UNIDADE"].astype(str).fillna("")
                    if str(u).strip()
                })[:120]
                st.write("UNIDADES (limpas+normalizadas) presentes na base (amostra):", amostra)
            except Exception as e:
                st.write("Falha no diagnÃ³stico:", e)
        st.warning("NÃ£o hÃ¡ registros compatÃ­veis com a(s) sua(s) palavra(s)-chave na base.")
        st.stop()

    st.info("Visualizando somente a(s) famÃ­lia(s): " + ", ".join(sorted(set(allowed_units))))
# ---------- fim do filtro ----------

# ===== PrÃ©via =====
st.subheader("PrÃ©-visualizaÃ§Ã£o da base (todas as colunas)")
st.dataframe(base.head(300), use_container_width=True, height=420)

# ===== Filtro global por UNIDADE =====
st.subheader("Filtro por UNIDADE")
units = sorted(base["UNIDADE"].dropna().astype(str).unique()) if "UNIDADE" in base.columns else []
default_sel = units[:1] if units else []
sel = st.multiselect("Selecione a(s) UNIDADE(s)", options=units, default=default_sel)

if "unit_filter_on" not in st.session_state:
    st.session_state["unit_filter_on"] = False

c1, c2, _ = st.columns([1,1,3])
with c1:
    if st.button("Aplicar filtro por UNIDADE"):
        st.session_state["unit_filter_on"] = True
with c2:
    if st.button("Limpar filtro"):
        st.session_state["unit_filter_on"] = False

base_filtrada = base.copy()
if st.session_state["unit_filter_on"] and sel:
    base_filtrada = base_filtrada[base_filtrada["UNIDADE"].isin(sel)]

st.write(f"Linhas apÃ³s filtro por UNIDADE: {len(base_filtrada)}")
st.dataframe(base_filtrada.head(300), use_container_width=True, height=420)

# Fonte para relatÃ³rios
df_fonte_relatorios = base_filtrada if (st.session_state["unit_filter_on"] and sel) else base

# ===== RelatÃ³rios especÃ­ficos =====
st.subheader("RelatÃ³rios especÃ­ficos (mantendo TODAS as colunas)")
masks, found_cols = build_requested_filters(df_fonte_relatorios)
if found_cols:
    st.caption("Colunas detectadas (cÃ³digo â†’ nome real): " + ", ".join([f"{k}â†’{v}" for k,v in found_cols.items()]))
else:
    st.info("AR/AS/AT/AU/AV/AN/AJ nÃ£o localizadas por letra/nome â€” verifique o cabeÃ§alho.")

tables_spec: Dict[str, pd.DataFrame] = {}
for code in ["AR","AS","AT","AU","AV","AN","AJ"]:
    if code in masks:
        df_tab = df_fonte_relatorios[masks[code]].copy()
        tables_spec[code] = df_tab
        with st.expander(f"{LABELS[code]} â€” {len(df_tab)} linha(s)"):
            st.dataframe(df_tab.head(300), use_container_width=True, height=360)
            cdl1, cdl2 = st.columns(2)
            with cdl1:
                st.download_button(f"Baixar {code}.csv",
                                   data=df_to_csv_bytes(df_tab),
                                   file_name=f"{code}.csv",
                                   mime="text/csv",
                                   key=f"csv_{code}")
            with cdl2:
                st.download_button(f"Baixar {code}.xlsx",
                                   data=single_sheet_xlsx(df_tab, sheet_name=code),
                                   file_name=f"{code}.xlsx",
                                   mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                   key=f"xlsx_{code}")
    else:
        st.info(f"Filtro {LABELS.get(code, code)}: coluna {code} nÃ£o localizada.")

# ===== AnÃ¡lise por critÃ©rio + grÃ¡fico por unidade =====
st.subheader("AnÃ¡lise por critÃ©rio e Unidade de SaÃºde")

def _norm(s: str) -> str:
    s = unicodedata.normalize("NFKD", str(s))
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    return re.sub(r"\s+", " ", s).strip().lower()

def resolve_crit_key(selection_text: str) -> Optional[str]:
    sel_n = _norm(selection_text)
    for code, spec in COL_SPECS.items():
        if sel_n == _norm(spec.get("desc","")): return code
        for nm in spec.get("names", []):
            if sel_n == _norm(nm): return code
        if _norm(spec.get("desc","")) in sel_n: return code
        for nm in spec.get("names", []):
            if _norm(nm) in sel_n: return code
    return None

crit_options = sorted({ (spec.get("names") or [spec.get("desc", code)])[0] for code, spec in COL_SPECS.items() },
                      key=lambda x: _norm(x))

crit_selecionado = st.selectbox(
    "Escolha o critÃ©rio",
    options=crit_options,
    index=0,
    help="Os critÃ©rios mapeiam para AR/AS/AT/AU/AV/AN/AJ conforme o cabeÃ§alho das planilhas."
)

crit_key = resolve_crit_key(crit_selecionado)

if not crit_key:
    st.warning("CritÃ©rio nÃ£o reconhecido (sem mapeamento).")
elif crit_key not in masks:
    st.warning(f"O critÃ©rio {crit_key} nÃ£o foi localizado nas planilhas.")
else:
    df_crit = df_fonte_relatorios[masks[crit_key]].copy()
    unidades_crit = sorted(df_crit["UNIDADE"].dropna().astype(str).unique())
    sel_unid_crit = st.multiselect("Filtrar por Unidade de SaÃºde (apenas para este critÃ©rio)",
                                   options=unidades_crit, default=unidades_crit)
    if sel_unid_crit:
        df_crit = df_crit[df_crit["UNIDADE"].isin(sel_unid_crit)]

    st.write(f"{LABELS[crit_key]} â€” total: {len(df_crit)} registro(s)")

    with st.expander("Ver linhas deste critÃ©rio"):
        st.dataframe(df_crit.head(300), use_container_width=True, height=320)
        st.download_button("Baixar CSV (critÃ©rio filtrado)",
                           data=df_to_csv_bytes(df_crit),
                           file_name=f"{crit_key}_criterio_filtrado.csv",
                           mime="text/csv")

    if df_crit.empty:
        st.info("Nenhum registro para este critÃ©rio com o filtro selecionado.")
    else:
        grp = df_crit.groupby("UNIDADE").size().reset_index(name="quantidade")
        ordem_final = sorted(grp["UNIDADE"].tolist())
        grp["UNIDADE"] = pd.Categorical(grp["UNIDADE"], categories=ordem_final, ordered=True)
        grp = grp.sort_values("UNIDADE")
        st.caption(f"GrÃ¡fico â€” {LABELS[crit_key]} por Unidade de SaÃºde")
        chart = alt.Chart(grp).mark_bar().encode(
            x=alt.X("UNIDADE:O", sort=ordem_final, title="Unidade de SaÃºde"),
            y=alt.Y("quantidade:Q", title="Quantidade"),
            tooltip=[alt.Tooltip("UNIDADE:O", title="Unidade"),
                     alt.Tooltip("quantidade:Q", title="Quantidade")]
        )
        st.altair_chart(chart.properties(height=420, width="container"), use_container_width=True)

# ===== Resumo e comparativo por prefixo (opcional) =====
TARGET_PREFIXES = [
    "ALTO DO EUCALIPTO","ALTO JOSE BONIFACIO","ESF MAIS BRUNO MAIA","ESF MAIS CORREGO DA BICA","BOLA NA REDE",
    "ALTO DA BRASILEIRA","CORREGO DO EUCALIPTO","ESF MAIS CORREGO JENIPAPO","ESF MAIS DOM HELDER","GUABIRABA",
    "INA ROSA","ESF MAIS MACAXEIRA","MANGABEIRA","ESF MAIS MARIA RITA","MARIO MONTEIRO","ESF MAIS MOACYR",
    "MORRO DA CONCEICAO","PASSARINHO ALTO","PASSARINHO BAIXO","ALTO JOSE DO PINHO","ALTO DO RESERVATORIO",
    "SANTA TEREZA","SITIOS DOS MACACOS"
]
def _norm_prefix(s: str) -> str:
    s = unicodedata.normalize("NFKD", str(s))
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    return re.sub(r"\s+", " ", s).strip().lower()
TARGET_PREFIXES_N = [_norm_prefix(p) for p in TARGET_PREFIXES]
def _matches_any_prefix(u: str) -> bool:
    return any(_norm_prefix(u).startswith(pn) for pn in TARGET_PREFIXES_N)
def unit_to_bucket(u: str) -> Optional[str]:
    nu = _norm_prefix(u)
    for raw_prefix, norm_prefix in zip(TARGET_PREFIXES, TARGET_PREFIXES_N):
        if nu.startswith(norm_prefix): return raw_prefix
    return None

st.subheader("Resumo â€” grupo de unidades (nomes com mesmo inÃ­cio)")
if "UNIDADE" not in df_fonte_relatorios.columns:
    st.warning("NÃ£o hÃ¡ coluna UNIDADE para agregar no resumo.")
else:
    df_grupo = df_fonte_relatorios[
        df_fonte_relatorios["UNIDADE"].astype(str).apply(_matches_any_prefix)
    ].copy()

    if df_grupo.empty:
        st.info("Nenhuma linha pertence ao grupo de unidades (por prefixo).")
    else:
        codigos_para_resumo = ["AR","AS","AT","AU","AV","AN","AJ"]
        contagens = []
        idx_grupo = df_grupo.index
        for code in codigos_para_resumo:
            if code in masks:
                mask_total = masks[code]
                total = int(mask_total.loc[mask_total.index.intersection(idx_grupo)].sum())
            else:
                total = 0
            contagens.append({"codigo":code,"criterio":LABELS.get(code,code),"quantidade":total})
        df_resumo = pd.DataFrame(contagens).sort_values("codigo")

        with st.expander("Tabela de totais no grupo de unidades (por prefixo)"):
            st.dataframe(df_resumo[["criterio","quantidade"]], use_container_width=True, height=300)
            st.download_button("Baixar CSV (resumo do grupo por prefixo)",
                               data=df_to_csv_bytes(df_resumo[["criterio","quantidade"]]),
                               file_name="resumo_grupo_unidades_prefixo.csv",
                               mime="text/csv")

        st.caption("GrÃ¡fico â€” Totais por critÃ©rio (grupo de unidades por prefixo)")
        chart_resumo = alt.Chart(df_resumo).mark_bar().encode(
            x=alt.X("criterio:N", sort=df_resumo["criterio"].tolist(), title="CritÃ©rio"),
            y=alt.Y("quantidade:Q", title="Quantidade"),
            tooltip=[alt.Tooltip("criterio:N", title="CritÃ©rio"),
                     alt.Tooltip("quantidade:Q", title="Quantidade")]
        )
        st.altair_chart(chart_resumo.properties(height=420, width="container"), use_container_width=True)

        st.subheader("Comparativo por unidade (agrupada por prefixo) â€” todos os critÃ©rios")
        df_all = df_fonte_relatorios.copy()
        df_all["bucket"] = df_all["UNIDADE"].astype(str).apply(unit_to_bucket)
        df_all = df_all[df_all["bucket"].notna()]

        if df_all.empty:
            st.info("Nenhuma linha pertence ao grupo de unidades (por prefixo).")
        else:
            linhas = []
            for code in codigos_para_resumo:
                if code in masks:
                    idx_ok = masks[code].index.intersection(df_all.index)
                    df_code = df_all.loc[idx_ok]
                    grp_b = df_code.groupby("bucket").size().reset_index(name="quantidade")
                else:
                    grp_b = pd.DataFrame({"bucket":sorted(df_all["bucket"].unique()),"quantidade":0})
                grp_b["codigo"] = code
                grp_b["criterio"] = LABELS.get(code, code)
                linhas.append(grp_b)

            df_barras = pd.concat(linhas, ignore_index=True) if linhas else pd.DataFrame(
                columns=["bucket","quantidade","codigo","criterio"]
            )
            # >>> garante tipo numÃ©rico para o eixo Y
            df_barras["quantidade"] = pd.to_numeric(df_barras["quantidade"], errors="coerce").fillna(0)

            ordem_buckets = [b for b in TARGET_PREFIXES if b in df_barras["bucket"].unique().tolist()]
            restantes = [b for b in df_barras["bucket"].unique().tolist() if b not in ordem_buckets]
            ordem_buckets = ordem_buckets + sorted(restantes)

            with st.expander("Tabela â€” contagens por unidade (consolidada por prefixo) e critÃ©rio"):
                st.dataframe(df_barras, use_container_width=True, height=360)
                st.download_button("Baixar CSV (unidade consolidada Ã— critÃ©rio)",
                                   data=df_to_csv_bytes(df_barras),
                                   file_name="comparativo_unidades_consolidadas.csv",
                                   mime="text/csv")

            # >>> canais corretos (XOffset) + tooltips com title=
            chart_buckets = alt.Chart(df_barras).mark_bar().encode()
            x=alt.X("bucket:O", sort=ordem_buckets, title="Unidade (agrupada por prefixo)"),
            y=alt.Y("quantidade:Q", title="Quantidade"),
            color=alt.Color("criterio:N", title="CritÃ©rio"),
            xOffset=alt.XOffset("criterio:N"),
                
            chart_buckets = alt.Chart(df_barras).mark_bar().encode(
    x=alt.X("bucket:O", sort=ordem_buckets, title="Unidade (agrupada por prefixo)"),
    y=alt.Y("quantidade:Q", title="Quantidade"),
    color=alt.Color("criterio:N", title="CritÃ©rio"),
    xOffset=alt.XOffset("criterio:N"),  # <- importante: XOffset, nÃ£o X
    tooltip=[
        alt.Tooltip("bucket:O",     title="Unidade"),
        alt.Tooltip("criterio:N",   title="CritÃ©rio"),
        alt.Tooltip("quantidade:Q", title="Quantidade"),
    ],
)
st.altair_chart(chart_buckets.properties(height=440, width="container"), use_container_width=True)

             